# a5

## 1. Character-based convolutional encoder for NMT

### (a)

Shape of the weight matrix, $\mathbf{W} \in \mathbb{R}^{f \times e_{\textrm{char}} \times k}$ is independent of the length of the input sequences, $m_{\textrm{word}}$. The shape of the output, $\mathbf{x}_{\textrm{conv}}$ does depend on the length of the input sequences, but we keep only the maximum over the input length dimension in the MaxPool layer.

### (b)

For kernel size $k = 5$, padding should be set to at least 2 because that would guarantee one window for the minimum $m_{\textrm{word}} = 1$.

### (c)



### (d) Describe two advantages of a Transformer encoder over the LSTM-with-attention encoder in out NMT model.

### (f)



### (g)



## 2. Character-based LSTM decoder for NMT

**BLEU Score**: 

## 3. Analyzing NMT Systems

### (a)

| word      | id    |
| --------- | ----- |
| traducir  | 4580  |
| traduzco  | 47197 |
| traduces  |       |
| traduce   | 7881  |
| traduzca  | 44704 |
| traduzcas |       |

This is unfortunate for a word-based NMT system since it will not be able to learn the infrequent forms as well even though it encounters related forms often. Character-aware NMT may overcome this problem by learning how to transform verbs into the different forms. For instance, it may learn to associate the roots "traduc" and "traduz" with the semantics "to translate" and extrapolate from the verb-endings what form the word is in. 

### (b)

#### i.

- **financial**: economic (0.337)
- **neuron**: neurons (0.439)
- **Francisco**: san (0.171)
- **naturally**: occuring (0.447)
- **expectation**: operator (0.552)

#### ii.

- **financial**: vertical (0.301)
- **neuron**: Newton (0.354)
- **Francisco**: France (0.420)
- **naturally**: practically (0.302)
- **expectation**: exception (0.389)

#### iii.

Word2Vec models semantic similarity; that is nearby words in terms of Word2Vec tend to be words that are used in similar context or appear together in a sentence. Similarly modeled by CharCNN tend 

contextual proximity



### (c)

