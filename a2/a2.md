# a2: Understanding word2vec

## 1.

### (a)

$\mathbf{y}$ is a one-hot vector with $y_w = \delta_o$, so $y_o \log(\hat{y}_o)$ is the only nonzero term in the sum.

### (b)

$\begin{align} \frac{\partial J}{\partial \mathbf{v}_c} &= - \mathbf{u}_o + \sum_{w \in \textrm{Vocab}} \hat{y}_w \mathbf{u}_w \end{align}$.

### (c)

$\begin{align} \frac{\partial J}{\partial \mathbf{u}_w} = \hat{y}_w \mathbf{v}_c \end{align}$ for $w \neq o$,

$\begin{align} \frac{\partial J}{\partial \mathbf{u}_o} &= -(1 - \hat{y}_o) \mathbf{v}_c \end{align}$.

### (d)

$\begin{align} \sigma'(x) &= \frac{d}{dx} \left( e^x (e^x + 1)^{-1} \right) \\ &= e^x (e^x + 1)^{-1} - e^x (e^x + 1)^{-2} \\ &= \sigma(x)(1 - \sigma(x)) \end{align}$.

### (e)

$\begin{align} \frac{\partial J}{\partial \mathbf{u}_o} = - (1 - \sigma(\mathbf{u}_o^T \mathbf{v}_c)) \mathbf{v}_c + \sum_{k = 1}^K (1 - \sigma(-\mathbf{u}_k^T \mathbf{v}_c)) \mathbf{u}_k \end{align}$.

$\begin{align} \frac{\partial J}{\partial \mathbf{u}_o} = - (1 - \sigma(\mathbf{u}_o^T \mathbf{v}_c)) \mathbf{v}_c \end{align}$.

$\begin{align} \frac{\partial J}{\partial \mathbf{u}_k} = (1 - \sigma(-\mathbf{u}_k^T \mathbf{v}_c)) \mathbf{v}_c \end{align}$ for $k \in [1,K]$.

For the naive-softmax loss we need to take a softmax over the entire vocabulary whereas in doing negative sampling we inspect K samples.

### (f)

(i) $\partial \mathbf{J}_{\mathrm{skip-gram}}(\mathbf{v}_c, w_{t-m}, \dots, w_{t+m}, \mathbf{U}) / \partial \mathbf{v}_w = \sum_{\substack{-m \leq j \leq m \\ j \neq 0}} \partial \mathbf{J}(\mathbf{v}_c, w_{t+j}, \mathbf{U}) / \partial \mathbf{U}_c$

(ii) $\partial \mathbf{J}_{\mathrm{skip-gram}}(\mathbf{v}_c, w_{t-m}, \dots, w_{t+m}, \mathbf{U}) / \partial \mathbf{v}_c = \sum_{\substack{-m \leq j \leq m \\ j \neq 0}} \partial \mathbf{J}(\mathbf{v}_c, w_{t+j}, \mathbf{U}) / \partial \mathbf{v}_c$

(iii) $\partial \mathbf{J}_{\mathrm{skip-gram}}(\mathbf{v}_c, w_{t-m}, \dots, w_{t+m}, \mathbf{U}) / \partial \mathbf{v}_w = 0$ when $w \neq c$.

