# a3: Dependency Parsing

## 1.

### (a)

#### i.

With momentum $\mathbf{m}$, the parameters are updated by the weighted average of previous $\mathbf{m}$ with the gradient, so the parameter updates are not only affected by the gradient but also the "momentum" direction of all previous updates. Having momentum helps with getting out of flat plateaux of the loss surface and mitigate the affects of noisy stochastic gradients.

#### ii.

By dividing by $\sqrt{\mathbf{v}}$, parameters with large gradients or frequent updates receive smaller effective updates whereas parameters with smaller gradients or infrequent updates receive larger effective updates. This helps the model learn all parameters simultaneously. For instance, at a saddle point SGD has a hard time breaking symmetry while optimizers with normalization proceed towards the minimum.

![optimizers@saddle point](./opt1.gif)

Images credit: Alec Radford.

### (b)

#### i.

$\gamma = 1 / (1 - p_{\textrm{drop}})$. Notice that $\mathbb{E}_{p_{\textrm{drop}}}[\mathbf{h_{\textrm{drop}}}]_i = (1 - p_{\textrm{drop}}) \gamma h_i = h_i$ and solve for $\gamma$.

#### ii.

Dropout during training can be interpreted as sampling a sub-network. During testing we do not apply dropout with the interpretation that we are evaluating the averaged prediction across the ensemble of all sub-networks.

## 2.

### (a)



### (b)



### (c)



### (d)



### (e)



### (f)

#### i.



#### ii.



#### iii.



#### iv.

