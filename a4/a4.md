# a4

## 1. Neural machine translation with RNNs

### (g)

It sets the corresponding components of attention scores, $\mathbf{e}_t$ to `-float('inf')`. Then, through softmaxing the components of attention distribution, $\alpha_t$ corresponding to 'pad' tokens are set to zero. Attention output, $\mathbf{a}_t$ also has those components set to zero. The 'pad' tokens do not contain useful information so we do not want the attention mechanism to pay attention to them.

### (i)

![BLEU test score](./bleu.png)

Corpus BLEU: **35.53**

### (j)

-  Dot product attention compared to multiplicative attention
  - Advantange: Simplicity. Does not require learned parameters.
  - Disadvantage: Encoder and Decoder needs to have same dimensions for hidden states; multiplicative attention can learn more complicated attention weighing schemes.

- Additive attention compared to multiplicative attention
  - Advantage: Can learn more complicated interaction between encoder hidden states and decoder state through separately learned parameters $\mathbf{W}_1$ and $\mathbf{W}_2$.
  - Disadvantage: Applying $\tanh$ to vectors is less efficient to compute than matrix multiplications.

As a matter of fact, additive attention seem to perform better than multiplicative attention:

> While the two are similar in theoretical complexity, dot-product attention is much faster and more space-efficient in practice, since it can be implemented using highly optimized matrix multiplication code.
>
>  While for small values of $d_k$ the two mechanisms perform similarly, additive attention outperforms dot product attention without scaling for larger values of $d_k$. We suspect that for large values of $d_k$, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients
>
> <cite>(Vaswani et al. 2017)</cite>

## 2. Analyzing NMT systems

### (a)

#### i.

1. "otro de mis favoritos" is mis-translated as "another favorite of my favorites".
2. Idiomatic phrase where a word in the correct translation is implied in the source.
3. Add more data that contain phrases that have one-to-many relations between the source words and target words.

#### ii.

1. The phrase "el autor para ni˜nos, ms ledo" is translated as "author for children, more reading."
2. The model does not recognize Spanish word ordering that is the opposite of that in English.
3. To recognize inverted word ordering, encode the source using bidirectional RNNs.

#### iii.

1. "Bolingbroke" is translated to "<unk>".
2. By default, out-of-vocabulary word is translated to "<unk>" token.
3. Try a hybrid system that switches to character-based NMT for "<unk>" token.

#### iv.

1. "manzana" is translated as "apple."
2. "manzana" in Spanish spoken in Spain is a homynym that can mean both "apple" and "city block."
3. Add examples where "manzana" is used to mean a "city block."

#### v.

1. "Sala de profesores" is mis-translated into "teachers' lounge."
2. Attention mechanism may erroneously be paying attention to "Ella" at the beginning of the sentence.
3. Try different attention schemes such as additive or normalized dot product.

#### vi.

1. Mis-translation of "100,000 hectáreas."
2. Conversion of SI units into Imperial/US units.
3. Augment the NMT system with handcrafted conversion rules for numerics with units.

### (b)



### (c)



## References

Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. “Attention Is All You Need.” In. https://arxiv.org/pdf/1706.03762.pdf.

